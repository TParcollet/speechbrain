# Generated 2020-09-17 from:
# /network/tmp1/ravanelm/speechbrain_github/speechbrain_g2p/recipes/LibriSpeech/G2P/hparams.yaml
# yamllint disable
# Seed needs to be set at top of yaml, before objects with parameters are made
# NOTE: Seed does not guarantee replicability with CTC
seed: 449
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

# Data files
output_folder: !ref results/g2p/<seed>
data_folder: /localscratch/LibriSpeech
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

input_lexicon: !ref <data_folder>/lexicon.csv
oov: !ref <data_folder>/oov.csv
wer_file: !ref <output_folder>/wer.txt

# These three files are created from lexicon.csv.
csv_train: !ref <data_folder>/lexicon_tr.csv
csv_valid: !ref <data_folder>/lexicon_dev.csv
csv_test: !ref <data_folder>/lexicon_test.csv

# Training hyperparameters
N_epochs: 15
N_batch: 1024
lr: 0.002
device: cuda:0

# token infomation
bos: 40
eos: 40

train_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
  csv_file: !ref <csv_train>
  batch_size: !ref <N_batch>
  csv_read: [graphemes, phonemes]
  sentence_sorting: random
  output_folder: !ref <output_folder>
  replacements:
    $data_folder: !ref <data_folder>

valid_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
  csv_file: !ref <csv_valid>
  batch_size: !ref <N_batch>
  csv_read: [graphemes, phonemes]
  sentence_sorting: ascending
  output_folder: !ref <output_folder>
  replacements:
    $data_folder: !ref <data_folder>

test_loader: !new:speechbrain.data_io.data_io.DataLoaderFactory
  csv_file: !ref <csv_test>
  batch_size: !ref <N_batch>
  csv_read: [graphemes, phonemes]
  sentence_sorting: ascending
  output_folder: !ref <output_folder>
  replacements:
    $data_folder: !ref <data_folder>

encoder_embed: !new:speechbrain.nnet.embedding.Embedding
  num_embeddings: 28    # 27 graphemes
  embedding_dim: 256

encoder_net: !new:speechbrain.nnet.RNN.LSTM
  bidirectional: true
  hidden_size: 512
  num_layers: 4
  dropout: 0.5

decoder_embed: !new:speechbrain.nnet.embedding.Embedding
  num_embeddings: 41    # 39 phonemes + 1 bos
  embedding_dim: 256

decoder_net: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder
  rnn_type: gru
  attn_type: content
  hidden_size: 512
  attn_dim: 256
  dropout: 0.5
  num_layers: 4

decoder_linear: !new:speechbrain.nnet.linear.Linear
  n_neurons: 41    # 39 phonemes + 1 eos
  bias: true

logsoftmax: !new:speechbrain.nnet.activations.Softmax
  apply_log: true

compute_cost: !name:speechbrain.nnet.losses.nll_loss

optimizer: !new:speechbrain.nnet.optimizers.Adam_Optimizer
  learning_rate: !ref <lr>

lr_annealing: !new:speechbrain.nnet.lr_schedulers.NewBobLRScheduler
  improvement_threshold: 0.0
  annealing_factor: 0.8
  patient: 0

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>
  summary_fns:
    loss: !name:speechbrain.utils.train_logger.summarize_average
    PER: !name:speechbrain.utils.train_logger.summarize_error_rate

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <N_epochs>
